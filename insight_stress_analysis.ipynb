{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a6e9462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (2.9.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: packaging in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: tensorflow_hub in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_hub) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_hub) (1.21.5)\n",
      "Requirement already satisfied: bert-tensorflow in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (1.0.4)\n",
      "Requirement already satisfied: six in /Users/mac/opt/anaconda3/lib/python3.9/site-packages (from bert-tensorflow) (1.16.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow.contrib (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow.contrib\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "!{sys.executable} -m pip install tensorflow_hub\n",
    "!{sys.executable} -m pip install bert-tensorflow\n",
    "!{sys.executable} -m pip install tensorflow.contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff033dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4448102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "970308d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## install bert model \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbert\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_classifier\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimization\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenization\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/bert/run_classifier.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m modeling\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimization\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenization\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/bert/modeling.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers \u001b[38;5;28;01mas\u001b[39;00m contrib_layers\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBertConfig\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m     33\u001b[0m   \u001b[38;5;124;03m\"\"\"Configuration for `BertModel`.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "## install bert model \n",
    "\n",
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86300977",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import dataset\n",
    "train = pd.read_csv('/Users/mac/Documents/GitHub/NatHacks2022/data/dreaddit-test.csv',encoding = \"ISO-8859-1\")\n",
    "test = pd.read_csv('/Users/mac/Documents/GitHub/NatHacks2022/data/dreaddit-train.csv',encoding = \"ISO-8859-1\")\n",
    "\n",
    "DATA_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'label'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c80031f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# transform dataset into a format understood by BERT\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Use the InputExample class from BERT's run_classifier code to create examples from the data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train_InputExamples \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: bert\u001b[38;5;241m.\u001b[39mrun_classifier\u001b[38;5;241m.\u001b[39mInputExample(guid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# Globally unique ID for bookkeeping, unused in this example\u001b[39;00m\n\u001b[1;32m      4\u001b[0m                                                                    text_a \u001b[38;5;241m=\u001b[39m x[DATA_COLUMN], \n\u001b[1;32m      5\u001b[0m                                                                    text_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m      6\u001b[0m                                                                    label \u001b[38;5;241m=\u001b[39m x[LABEL_COLUMN]), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m test_InputExamples \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: bert\u001b[38;5;241m.\u001b[39mrun_classifier\u001b[38;5;241m.\u001b[39mInputExample(guid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m      9\u001b[0m                                                                    text_a \u001b[38;5;241m=\u001b[39m x[DATA_COLUMN], \n\u001b[1;32m     10\u001b[0m                                                                    text_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m     11\u001b[0m                                                                    label \u001b[38;5;241m=\u001b[39m x[LABEL_COLUMN]), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# transform dataset into a format understood by BERT\n",
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49f2bc11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             vocab_file, do_lower_case \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun([tokenization_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_file\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     10\u001b[0m                                                 tokenization_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_lower_case\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bert\u001b[38;5;241m.\u001b[39mtokenization\u001b[38;5;241m.\u001b[39mFullTokenizer(vocab_file\u001b[38;5;241m=\u001b[39mvocab_file, \n\u001b[1;32m     13\u001b[0m                                     do_lower_case\u001b[38;5;241m=\u001b[39mdo_lower_case)\n\u001b[0;32m---> 15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_tokenizer_from_hub_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mcreate_tokenizer_from_hub_module\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_tokenizer_from_hub_module\u001b[39m():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mGraph()\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m      6\u001b[0m         bert_module \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mModule(BERT_MODEL_HUB)\n\u001b[1;32m      7\u001b[0m         tokenization_info \u001b[38;5;241m=\u001b[39m bert_module(signature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenization_info\u001b[39m\u001b[38;5;124m\"\u001b[39m, as_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Load a vocabulary file and lowercasing information directly from the BERT tf hub module\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, \n",
    "                                    do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c65029bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(text\u001b[38;5;241m.\u001b[39miloc[i])\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m max_len\n\u001b[0;32m----> 9\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m max_len \u001b[38;5;241m=\u001b[39m get_max_len(temp)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Set the maximum sequence length. \n",
    "def get_max_len(text):\n",
    "    max_len = 0\n",
    "    for i in range(len(train)):\n",
    "        if len(text.iloc[i]) > max_len:\n",
    "            max_len = len(text.iloc[i])\n",
    "    return max_len\n",
    "\n",
    "temp = train.text.str.split(' ')\n",
    "max_len = get_max_len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfbc07dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m MAX_SEQ_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[43mmax_len\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Convert our train and test features to InputFeatures that BERT understands.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m train_features \u001b[38;5;241m=\u001b[39m bert\u001b[38;5;241m.\u001b[39mrun_classifier\u001b[38;5;241m.\u001b[39mconvert_examples_to_features(train_InputExamples, \n\u001b[1;32m      4\u001b[0m                                                                   label_list, \n\u001b[1;32m      5\u001b[0m                                                                   MAX_SEQ_LENGTH, \n\u001b[1;32m      6\u001b[0m                                                                   tokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_len' is not defined"
     ]
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = max_len\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, \n",
    "                                                                  label_list, \n",
    "                                                                  MAX_SEQ_LENGTH, \n",
    "                                                                  tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, \n",
    "                                                                 label_list, \n",
    "                                                                 MAX_SEQ_LENGTH, \n",
    "                                                                 tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "181c1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n",
    "    bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    # Use \"sequence_outputs\" for token-level output.\n",
    "    output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    # Create our own layer to tune for politeness data.\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\"output_bias\", \n",
    "                                  [num_labels], \n",
    "                                  initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "\n",
    "        # Dropout helps prevent overfitting\n",
    "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Convert labels into one-hot encoding\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "        if is_predicting:\n",
    "            return (predicted_labels, log_probs)\n",
    "\n",
    "        # If we're train/eval, compute loss between predicted and actual label\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "            (loss, predicted_labels, log_probs) = create_model(is_predicting, \n",
    "                                                               input_ids, \n",
    "                                                               input_mask, \n",
    "                                                               segment_ids, \n",
    "                                                               label_ids, \n",
    "                                                               num_labels)\n",
    "\n",
    "            train_op = bert.optimization.create_optimizer(loss, \n",
    "                                                          learning_rate, \n",
    "                                                          num_train_steps, \n",
    "                                                          num_warmup_steps, \n",
    "                                                          use_tpu=False)\n",
    "\n",
    "            # Calculate evaluation metrics. \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics.f1_score(label_ids, predicted_labels)\n",
    "                auc = tf.metrics.auc(label_ids, predicted_labels)\n",
    "                recall = tf.metrics.recall(label_ids, predicted_labels)\n",
    "                precision = tf.metrics.precision(label_ids, predicted_labels) \n",
    "                true_pos = tf.metrics.true_positives(label_ids, predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(label_ids, predicted_labels)   \n",
    "                false_pos = tf.metrics.false_positives(label_ids, predicted_labels)  \n",
    "                false_neg = tf.metrics.false_negatives(label_ids, predicted_labels)\n",
    "\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"auc\": auc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg\n",
    "                }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(is_predicting, \n",
    "                                                         input_ids, \n",
    "                                                         input_mask, \n",
    "                                                         segment_ids, \n",
    "                                                         label_ids, \n",
    "                                                         num_labels)\n",
    "\n",
    "            predictions = {'probabilities': log_probs, 'labels': predicted_labels}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cfcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf06b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "OUTPUT_DIR = 'output'\n",
    "\n",
    "run_config = tf.estimator.RunConfig(model_dir=OUTPUT_DIR,\n",
    "                                    save_summary_steps=SAVE_SUMMARY_STEPS,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e27216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(num_labels=len(label_list), \n",
    "                            learning_rate=LEARNING_RATE,\n",
    "                            num_train_steps=num_train_steps,\n",
    "                            num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
    "                                   config=run_config,\n",
    "                                   params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(features=train_features,\n",
    "                                                      seq_length=MAX_SEQ_LENGTH,\n",
    "                                                      is_training=True,\n",
    "                                                      drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc531d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "\n",
    "# train the model \n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the test result\n",
    "test_input_fn = run_classifier.input_fn_builder(features=test_features,\n",
    "                                                seq_length=MAX_SEQ_LENGTH,\n",
    "                                                is_training=False,\n",
    "                                                drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0398a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(in_sentences):\n",
    "    labels = [\"non-stress\", \"stress\"]\n",
    "    labels_idx = [0, 1]\n",
    "    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "\n",
    "    input_features = run_classifier.convert_examples_to_features(input_examples, \n",
    "                                                                 labels_idx, \n",
    "                                                                 MAX_SEQ_LENGTH, \n",
    "                                                                 tokenizer)\n",
    "    \n",
    "    predict_input_fn = run_classifier.input_fn_builder(features=input_features, \n",
    "                                                       seq_length=MAX_SEQ_LENGTH, \n",
    "                                                       is_training=False, \n",
    "                                                       drop_remainder=False)\n",
    "\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    return [{\"text\": sentence, \"confidence\": list(prediction['probabilities']), \"labels\": labels[prediction['labels']]}\n",
    "            for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
